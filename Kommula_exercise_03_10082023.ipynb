{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhanuprasdkommula/INFO_5810/blob/main/Kommula_exercise_03_10082023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "## The third In-class-exercise (due on 11:59 PM 10/08/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2htC-oV70ne"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "7b64f298-1025-478c-d63b-cd9d5ae93242"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nPlease write you answer here:\\n\\nAn interesting text classification task could be sentiment analysis of customer reviews for a product or service. The goal is to classify each review as positive, negative, or neutral based on the sentiment expressed by the customer. To build a machine learning model for this task, the following features could be useful:\\nBag of Words (BoW):\\n\\nA BoW model expresses text data as numeric characteristics by first constructing a vocabulary of known terms in the corpus and then constructing a vector for each document that contains counts of how frequently each word appears.\\n\\nThis feature records the frequency of positive and negative words in the reviews and feedback helping the model understand the overall sentiment of the customer\\n\\nTerm Frequency-Inverse Document Frequency (TF-IDF): \\nThe TF-IDF technique weights words in a document according to their rarity in the entire corpus and their frequency in the document.\\n\\nWith the use of this feature, the model may be able to concentrate on terms that are more useful for sentiment analysis, such as details about a product or strong positive or negative words.\\n\\nN-grams:\\n\\nContiguous sequences of n items from a given sample of text or audio are known as n-grams.\\n\\nUsing bi-grams or tri-grams as features in sentiment analysis helps capture the relationship between words and provide the model more context to comprehend the sentiment represented in the reviews.\\n\\nWord Embeddings:\\n\\nDense vector word representations known as word embeddings capture the semantic links between words.\\n\\nPre-trained word embeddings can be utilized as features to provide the model with a more nuanced grasp of the meaning of words in the reviews, which can assist in increasing the precision of the sentiment analysis. Examples of these embeddings include Word2Vec and GloVe.\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "An interesting text classification task could be sentiment analysis of customer reviews for a product or service. The goal is to classify each review as positive, negative, or neutral based on the sentiment expressed by the customer. To build a machine learning model for this task, the following features could be useful:\n",
        "Bag of Words (BoW):\n",
        "\n",
        "A BoW model expresses text data as numeric characteristics by first constructing a vocabulary of known terms in the corpus and then constructing a vector for each document that contains counts of how frequently each word appears.\n",
        "\n",
        "This feature records the frequency of positive and negative words in the reviews and feedback helping the model understand the overall sentiment of the customer\n",
        "\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF):\n",
        "The TF-IDF technique weights words in a document according to their rarity in the entire corpus and their frequency in the document.\n",
        "\n",
        "With the use of this feature, the model may be able to concentrate on terms that are more useful for sentiment analysis, such as details about a product or strong positive or negative words.\n",
        "\n",
        "N-grams:\n",
        "\n",
        "Contiguous sequences of n items from a given sample of text or audio are known as n-grams.\n",
        "\n",
        "Using bi-grams or tri-grams as features in sentiment analysis helps capture the relationship between words and provide the model more context to comprehend the sentiment represented in the reviews.\n",
        "\n",
        "Word Embeddings:\n",
        "\n",
        "Dense vector word representations known as word embeddings capture the semantic links between words.\n",
        "\n",
        "Pre-trained word embeddings can be utilized as features to provide the model with a more nuanced grasp of the meaning of words in the reviews, which can assist in increasing the precision of the sentiment analysis. Examples of these embeddings include Word2Vec and GloVe.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60787e66-a56b-44d1-bf3d-697d91b513ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words (BoW) features:\n",
            "   an  and  average  great  is  it  more  not  nothing  product  regular  \\\n",
            "0   0    0        0      1   1   0     0    0        0        1        0   \n",
            "1   0    0        0      0   0   0     0    1        0        1        0   \n",
            "2   1    1        1      0   0   1     1    0        1        1        1   \n",
            "\n",
            "   satisfied  special  the  with  \n",
            "0          0        0    1     0  \n",
            "1          1        0    1     1  \n",
            "2          0        1    0     0  \n",
            "TF-IDF features:\n",
            "         an       and   average     great        is        it      more  \\\n",
            "0  0.000000  0.000000  0.000000  0.584483  0.584483  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2  0.346089  0.346089  0.346089  0.000000  0.000000  0.346089  0.346089   \n",
            "\n",
            "        not   nothing   product   regular  satisfied   special       the  \\\n",
            "0  0.000000  0.000000  0.345205  0.000000   0.000000  0.000000  0.444514   \n",
            "1  0.504611  0.000000  0.298032  0.000000   0.504611  0.000000  0.383770   \n",
            "2  0.000000  0.346089  0.204405  0.346089   0.000000  0.346089  0.000000   \n",
            "\n",
            "       with  \n",
            "0  0.000000  \n",
            "1  0.504611  \n",
            "2  0.000000  \n",
            "N-grams features:\n",
            "   an average  and regular  average and  is great  it an  more special  \\\n",
            "0           0            0            0         1      0             0   \n",
            "1           0            0            0         0      0             0   \n",
            "2           1            1            1         0      1             1   \n",
            "\n",
            "   not satisfied  nothing more  product is  product nothing  regular product  \\\n",
            "0              0             0           1                0                0   \n",
            "1              1             0           0                0                0   \n",
            "2              0             1           0                1                1   \n",
            "\n",
            "   satisfied with  the product  with the  \n",
            "0               0            1         0  \n",
            "1               1            1         1  \n",
            "2               0            0         0  \n",
            "Word Embeddings features:\n",
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.001800  0.007046  0.002945 -0.006981  0.007713 -0.005989  0.008998   \n",
            "1  -0.000536  0.000236  0.005103  0.009009 -0.009303 -0.007117  0.006459   \n",
            "2  -0.008243  0.009299 -0.000198 -0.001967  0.004604 -0.004095  0.002743   \n",
            "3  -0.007139  0.001241 -0.007177 -0.002245  0.003719  0.005833  0.001198   \n",
            "4  -0.008728  0.002130 -0.000874 -0.009319 -0.009428 -0.001411  0.004432   \n",
            "5   0.008133 -0.004458 -0.001068  0.001006 -0.000191  0.001148  0.006114   \n",
            "6   0.008168 -0.004443  0.008985  0.008254 -0.004435  0.000303  0.004274   \n",
            "7  -0.009579  0.008943  0.004165  0.009235  0.006644  0.002925  0.009804   \n",
            "8  -0.005153 -0.006670 -0.007777  0.008312 -0.001982 -0.006855 -0.004153   \n",
            "9  -0.000536  0.000236  0.005103  0.009009 -0.009303 -0.007117  0.006459   \n",
            "10  0.007088 -0.001567  0.007947 -0.009490 -0.008030 -0.006641 -0.004003   \n",
            "11  0.009770  0.008165  0.001281  0.005098  0.001408 -0.006455 -0.001428   \n",
            "12 -0.001944 -0.005268  0.009447 -0.009299  0.004504  0.005404 -0.001409   \n",
            "13 -0.009499  0.009562 -0.007772 -0.002646 -0.004906 -0.004968 -0.008022   \n",
            "14  0.007696  0.009124  0.001135 -0.008331  0.008423 -0.003698  0.005746   \n",
            "15 -0.007191  0.004233  0.002163  0.007441 -0.004889 -0.004564 -0.006098   \n",
            "16  0.001300 -0.009804  0.004588 -0.000538  0.006332  0.001783 -0.003130   \n",
            "17  0.000095  0.003078 -0.006813 -0.001377  0.007669  0.007345 -0.003672   \n",
            "18 -0.008619  0.003665  0.005190  0.005742  0.007467 -0.006168  0.001106   \n",
            "\n",
            "          7         8         9   ...        90        91        92        93  \\\n",
            "0   0.002959 -0.004015 -0.004689  ...  0.009129  0.003587  0.006565 -0.003611   \n",
            "1   0.008973 -0.005015 -0.003763  ...  0.001631  0.000190  0.003474  0.000218   \n",
            "2   0.006940  0.006065 -0.007511  ... -0.007426 -0.001064 -0.000795 -0.002563   \n",
            "3   0.002103 -0.004110  0.007225  ...  0.003137 -0.004713  0.005281 -0.004233   \n",
            "4   0.003704 -0.006499 -0.006873  ...  0.009071  0.008939 -0.008208 -0.003012   \n",
            "5  -0.000020 -0.003246 -0.001511  ... -0.002702  0.000444 -0.003538 -0.000419   \n",
            "6  -0.003926 -0.005560 -0.006512  ...  0.002058 -0.004004 -0.008241  0.006278   \n",
            "7  -0.004425 -0.006803  0.004227  ... -0.005085  0.001131  0.002883 -0.001536   \n",
            "8   0.005144 -0.002870 -0.003750  ... -0.008979  0.008592  0.004047  0.007470   \n",
            "9   0.008973 -0.005015 -0.003763  ...  0.001631  0.000190  0.003474  0.000218   \n",
            "10  0.004990 -0.003814 -0.008322  ...  0.007514  0.001499 -0.001265  0.005768   \n",
            "11  0.006449 -0.004617 -0.003993  ...  0.004774 -0.003262 -0.009268  0.003787   \n",
            "12  0.009007  0.009885 -0.005475  ...  0.002651 -0.002565  0.006448 -0.007660   \n",
            "13 -0.007783 -0.004554 -0.001278  ...  0.008381  0.007235  0.001730 -0.001347   \n",
            "14  0.004394  0.009689 -0.009299  ...  0.007102  0.001906  0.005196  0.006380   \n",
            "15  0.003299 -0.004499  0.008523  ... -0.003481  0.003491 -0.005796 -0.008750   \n",
            "16  0.007760  0.001555  0.000055  ...  0.007012  0.004829  0.008683  0.007094   \n",
            "17  0.002644 -0.008317  0.006203  ... -0.004508  0.005704  0.009180 -0.004099   \n",
            "18  0.006047 -0.002840 -0.006174  ...  0.001087 -0.001576  0.002197 -0.007881   \n",
            "\n",
            "          94        95        96        97        98        99  \n",
            "0   0.006793  0.007244 -0.002133 -0.001860  0.003612 -0.007036  \n",
            "1   0.009619  0.005061 -0.008917 -0.007042  0.000901  0.006393  \n",
            "2   0.009683 -0.000459  0.005874 -0.007448 -0.002506 -0.005550  \n",
            "3   0.002642 -0.008046  0.006210  0.004819  0.000787  0.003013  \n",
            "4   0.009887  0.005104 -0.001589 -0.008692  0.002962 -0.006676  \n",
            "5  -0.000709  0.000823  0.008196 -0.005737 -0.001660  0.005572  \n",
            "6  -0.001949 -0.000666 -0.001771 -0.004536  0.004062 -0.004270  \n",
            "7   0.009932  0.008350  0.002416  0.007118  0.005891 -0.005581  \n",
            "8   0.009745 -0.007290 -0.009037  0.005835  0.009390  0.003509  \n",
            "9   0.009619  0.005061 -0.008917 -0.007042  0.000901  0.006393  \n",
            "10 -0.005639  0.000039  0.009458 -0.005482  0.003814 -0.008112  \n",
            "11  0.007161 -0.005633 -0.007865 -0.002973 -0.004932 -0.002315  \n",
            "12  0.003394  0.000490  0.008732  0.005983  0.006815  0.007823  \n",
            "13 -0.005889 -0.004534  0.008650 -0.003136 -0.006339  0.009873  \n",
            "14  0.001916 -0.006126 -0.000004  0.008265 -0.006100  0.009439  \n",
            "15 -0.005516  0.006749  0.006418  0.009438  0.007055  0.006755  \n",
            "16 -0.005694  0.007241 -0.009295 -0.002588 -0.007757  0.004193  \n",
            "17  0.007966  0.005375  0.005881  0.000512  0.008212 -0.007017  \n",
            "18 -0.002717  0.002663  0.005347 -0.002392 -0.009510  0.004507  \n",
            "\n",
            "[19 rows x 100 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Dummy dataset of customer reviews/feedbacks\n",
        "feedbacks = [\n",
        "    \"The product is great...!\",\n",
        "    \"I'm not satisfied with the product\",\n",
        "    \"It's an average and regular product, nothing more special\"\n",
        "]\n",
        "\n",
        "# Bag of Words (BoW) feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "bow_features = vectorizer.fit_transform(feedbacks)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "bow_dataframe = pd.DataFrame(bow_features.toarray(), columns=feature_names)\n",
        "print(\"Bag of Words (BoW) features:\")\n",
        "print(bow_dataframe)\n",
        "\n",
        "# Term Frequency-Inverse Document Frequency (TF-IDF) feature extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(feedbacks)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_dataframe = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_feature_names)\n",
        "print(\"TF-IDF features:\")\n",
        "print(tfidf_dataframe)\n",
        "\n",
        "# N-grams feature extraction\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram_features = ngram_vectorizer.fit_transform(feedbacks)\n",
        "ngram_feature_names = ngram_vectorizer.get_feature_names_out()\n",
        "ngram_dataframe = pd.DataFrame(ngram_features.toarray(), columns=ngram_feature_names)\n",
        "print(\"N-grams features:\")\n",
        "print(ngram_dataframe)\n",
        "\n",
        "# Word Embeddings feature extraction\n",
        "word_embeddings = Word2Vec([review.split() for review in feedbacks], min_count=1)\n",
        "word_embeddings_dataframe = pd.DataFrame([word_embeddings.wv.get_vector(word) for review in feedbacks for word in review.split()])\n",
        "print(\"Word Embeddings features:\")\n",
        "print(word_embeddings_dataframe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc471cb4-1791-481d-874c-7b2e27a9cca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R-squared score on the test set: 0.59\n",
            "Selected features:\n",
            "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Latitude']\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Load the California housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize the k-NN regressor\n",
        "knn = KNeighborsRegressor()\n",
        "\n",
        "# Initialize SelectKBest for feature selection\n",
        "num_features_to_select = 5  # Choose the number of features you want to select\n",
        "select_k_best = SelectKBest(score_func=f_regression, k=num_features_to_select)\n",
        "\n",
        "# Fit SelectKBest on the training data and select the top features\n",
        "selected_X_train = select_k_best.fit_transform(X_train_scaled, y_train)\n",
        "\n",
        "# Get the selected feature indices\n",
        "selected_feature_indices = select_k_best.get_support(indices=True)\n",
        "\n",
        "# Get the selected feature names\n",
        "feature_names = data.feature_names\n",
        "selected_features = [feature_names[i] for i in selected_feature_indices]\n",
        "\n",
        "# Train k-NN on the selected features\n",
        "knn.fit(selected_X_train, y_train)\n",
        "\n",
        "# Transform the test data to include only the selected features\n",
        "selected_X_test = X_test_scaled[:, selected_feature_indices]\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_score = knn.score(selected_X_test, y_test)\n",
        "print(f\"R-squared score on the test set: {test_score:.2f}\")\n",
        "\n",
        "# Display the selected feature names\n",
        "print(\"Selected features:\")\n",
        "print(selected_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e4fad85-a696-47fe-fdbb-61151b7aa455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Feedbacks:\n",
            "Rank 1: Similarity Score=0.6907\n",
            "It's an average and regular product, nothing more special\n",
            "\n",
            "Rank 2: Similarity Score=0.6344\n",
            "I'm not satisfied with the product\n",
            "\n",
            "Rank 3: Similarity Score=0.5901\n",
            "The product is great...!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Dummy dataset of customer reviews/feedbacks\n",
        "feedbacks = [\n",
        "    \"The product is great...!\",\n",
        "    \"I'm not satisfied with the product\",\n",
        "    \"It's an average and regular product, nothing more special\"\n",
        "]\n",
        "\n",
        "# Your query\n",
        "query = \"Looking for a top-quality product\"\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = 'bert-base-uncased'  # You can change this to a different BERT model if needed\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize and obtain BERT embeddings for the query\n",
        "query_tokens = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "query_output = model(**query_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Tokenize and obtain BERT embeddings for the feedbacks\n",
        "feedback_tokens = tokenizer(feedbacks, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "feedback_output = model(**feedback_tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "# Calculate cosine similarity between the query and each feedback\n",
        "cosine_similarities = cosine_similarity(query_output.detach().numpy(), feedback_output.detach().numpy())\n",
        "\n",
        "# Rank the feedbacks based on similarity in descending order\n",
        "similarity_scores = cosine_similarities[0]  # Assuming only one query\n",
        "ranked_indices = np.argsort(similarity_scores)[::-1]\n",
        "ranked_feedbacks = [feedbacks[i] for i in ranked_indices]\n",
        "\n",
        "# Print the ranked feedbacks and their similarity scores\n",
        "print(\"Ranked Feedbacks:\")\n",
        "for i, feedback in enumerate(ranked_feedbacks):\n",
        "    print(f\"Rank {i+1}: Similarity Score={similarity_scores[ranked_indices[i]]:.4f}\")\n",
        "    print(feedback)\n",
        "    print()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}